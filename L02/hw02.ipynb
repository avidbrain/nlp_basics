{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52be49e4",
   "metadata": {},
   "source": [
    "# Введение в обработку естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99966612",
   "metadata": {},
   "source": [
    "## Урок 2. Создание признакового пространства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01768bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b41f3",
   "metadata": {},
   "source": [
    "## Данные Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67566e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_PROCESSED_PATH = \"../data/tweets.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64eeccdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "      <td>hillary campaigned today in ohio omg amp used ...</td>\n",
       "      <td>[hillary, campaigned, today, in, ohio, omg, am...</td>\n",
       "      <td>[hillary, campaigned, today, ohio, omg, amp, u...</td>\n",
       "      <td>[hillari, campaign, today, ohio, omg, amp, use...</td>\n",
       "      <td>[hillary, campaigned, today, ohio, omg, amp, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "      <td>happy at work conference right mindset leads t...</td>\n",
       "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
       "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
       "      <td>[happi, work, confer, right, mindset, lead, cu...</td>\n",
       "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "      <td>my song so glad free download shoegaze newmusi...</td>\n",
       "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
       "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
       "      <td>[song, glad, free, download, shoegaz, newmus, ...</td>\n",
       "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "49156  49157    NaN  #hillary #campaigned today in #ohio((omg)) &am...   \n",
       "49157  49158    NaN  happy, at work conference: right mindset leads...   \n",
       "49158  49159    NaN  my   song \"so glad\" free download!  #shoegaze ...   \n",
       "\n",
       "                                                    text  \\\n",
       "49156  hillary campaigned today in ohio omg amp used ...   \n",
       "49157  happy at work conference right mindset leads t...   \n",
       "49158  my song so glad free download shoegaze newmusi...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "49156  [hillary, campaigned, today, in, ohio, omg, am...   \n",
       "49157  [happy, at, work, conference, right, mindset, ...   \n",
       "49158  [my, song, so, glad, free, download, shoegaze,...   \n",
       "\n",
       "                                    tweet_token_filtered  \\\n",
       "49156  [hillary, campaigned, today, ohio, omg, amp, u...   \n",
       "49157  [happy, work, conference, right, mindset, lead...   \n",
       "49158  [song, glad, free, download, shoegaze, newmusi...   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "49156  [hillari, campaign, today, ohio, omg, amp, use...   \n",
       "49157  [happi, work, confer, right, mindset, lead, cu...   \n",
       "49158  [song, glad, free, download, shoegaz, newmus, ...   \n",
       "\n",
       "                                        tweet_lemmatized  \n",
       "49156  [hillary, campaigned, today, ohio, omg, amp, u...  \n",
       "49157  [happy, work, conference, right, mindset, lead...  \n",
       "49158  [song, glad, free, download, shoegaze, newmusi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = pd.read_pickle(TWITTER_PROCESSED_PATH)\n",
    "twitter_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682eb1c5",
   "metadata": {},
   "source": [
    "### Задание 1. Создать BoW с помощью CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7842c25",
   "metadata": {},
   "source": [
    "Применим векторайзер к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "-\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "-\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "-\tИсключим стоп-слова с помощью stop_words='english'. \n",
    "-\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92f85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    max_features = 1000,\n",
    "    ngram_range=(1,1),\n",
    "    analyzer='word', \n",
    "    binary=False,\n",
    "    preprocessor=lambda x: x,\n",
    "    tokenizer=lambda x: x,\n",
    "    max_df=0.9\n",
    "    #stop_words='english'  # уже сделано на этапе препроцессинга\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c23f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "3    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "4    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   year  yesterday  yet  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0          0    0   0     0     0      0       0   0      0  \n",
       "1     0          0    0   0     0     0      0       0   0      0  \n",
       "2     0          0    0   0     0     0      0       0   0      0  \n",
       "3     0          0    0   0     0     0      0       0   0      0  \n",
       "4     0          0    0   0     0     0      0       0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_stemmed = twitter_df['tweet_stemmed']\n",
    "bow_stemmed = count_vectorizer.fit_transform(documents_stemmed)\n",
    "words_stemmed = pd.DataFrame(bow_stemmed.toarray(),\n",
    "                             columns = count_vectorizer.get_feature_names())\n",
    "words_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1985aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0     0           0        0    0       0      0         0      0    0   \n",
       "1     0           0        0    0       0      0         0      0    0   \n",
       "2     0           0        0    0       0      0         0      0    0   \n",
       "3     0           0        0    0       0      0         0      0    0   \n",
       "4     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "   adventure  ...  yes  yesterday  yet  yo  yoga  york  young  youtube  yr  \\\n",
       "0          0  ...    0          0    0   0     0     0      0        0   0   \n",
       "1          0  ...    0          0    0   0     0     0      0        0   0   \n",
       "2          0  ...    0          0    0   0     0     0      0        0   0   \n",
       "3          0  ...    0          0    0   0     0     0      0        0   0   \n",
       "4          0  ...    0          0    0   0     0     0      0        0   0   \n",
       "\n",
       "   yummy  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_lemmatized = twitter_df['tweet_lemmatized']\n",
    "bow_lemmatized = count_vectorizer.fit_transform(documents_lemmatized)\n",
    "words_lemmatized = pd.DataFrame(bow_lemmatized.toarray(),\n",
    "                                columns = count_vectorizer.get_feature_names())\n",
    "words_lemmatized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9bd83",
   "metadata": {},
   "source": [
    "### Задание 2. Создать BoW с помощью TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75876ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аналогично заданию 1, но ради разнообразия восстановим сначала документы из списков токенов.\n",
    "documents_stemmed = documents_stemmed.apply(lambda tokens: ' '.join(tokens))\n",
    "documents_lemmatized = documents_lemmatized.apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb120d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features = 1000,\n",
    "    max_df=0.9,\n",
    "    stop_words='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db63ddc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "   yeah  year  yesterday   yo  yoga  york  young  youtub   yr  yummi  \n",
       "0   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "1   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "2   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "3   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "4   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_stemmed = tfidf_vectorizer.fit_transform(documents_stemmed)\n",
    "words_stemmed = pd.DataFrame(tfidf_stemmed.toarray(),\n",
    "                             columns = tfidf_vectorizer.get_feature_names())\n",
    "words_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2a0cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "1   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "2   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "3   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "4   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "\n",
       "   adventure  ...  year  yes  yesterday   yo  yoga  york  young  youtube   yr  \\\n",
       "0        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "1        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "2        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "3        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "4        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "\n",
       "   yummy  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_lemmatized = tfidf_vectorizer.fit_transform(documents_lemmatized)\n",
    "words_lemmatized = pd.DataFrame(tfidf_lemmatized.toarray(),\n",
    "                             columns = tfidf_vectorizer.get_feature_names())\n",
    "words_lemmatized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54fd01",
   "metadata": {},
   "source": [
    "### Задание 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d7dbd",
   "metadata": {},
   "source": [
    "3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab9189",
   "metadata": {},
   "source": [
    "**Примечание:** не совсем понятно, о каком корпусе идет речь, но раз продолжаем работать с базой твитов, то в качестве корпуса используем train-часть подготовленной в результате практической работы №1 таблицы twitter_df.\n",
    "\n",
    "В качестве PCA используем TruncatedSVD, как как чистый PCA не работает со sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e238e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents_lemmatized[~twitter_df.label.isna()]\n",
    "labels = twitter_df.label[~twitter_df.label.isna()].astype(int)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(documents, labels, random_state=100, shuffle=True, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd36861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(y_true, y_pred):\n",
    "    return (\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        balanced_accuracy_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d6c439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем базовые метрики: всегда предсказываем ноль\n",
    "model_performance = {'Baseline: all zeros': get_performance(valid_y, np.zeros(valid_y.size))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d75cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid = {    \n",
    "    'TF-IDF': make_pipeline(\n",
    "        TfidfVectorizer(), LogisticRegression()\n",
    "    ),\n",
    "    'TF-IDF (100 features)': make_pipeline(\n",
    "        TfidfVectorizer(max_features=100), LogisticRegression()\n",
    "    ),\n",
    "    'TF-IDF (1000 features)': make_pipeline(\n",
    "        TfidfVectorizer(max_features=1000), LogisticRegression()\n",
    "    ),\n",
    "    'TF-IDF (5000 features)': make_pipeline(\n",
    "        TfidfVectorizer(max_features=5000), LogisticRegression()\n",
    "    ),\n",
    "    'TF-IDF (10000 features)': make_pipeline(\n",
    "        TfidfVectorizer(max_features=10000), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer': make_pipeline(\n",
    "        CountVectorizer(), LogisticRegression()\n",
    "    ),    \n",
    "    'Count Vectorizer (1000 features)': make_pipeline(\n",
    "        CountVectorizer(max_features=1000), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer (5000 features)': make_pipeline(\n",
    "        CountVectorizer(max_features=5000), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer (10000 features)': make_pipeline(\n",
    "        CountVectorizer(max_features=10000), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer (15000 features)': make_pipeline(\n",
    "        CountVectorizer(max_features=15000), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer + TruncatedSVD(150)': make_pipeline(\n",
    "        CountVectorizer(), TruncatedSVD(150), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer + TruncatedSVD(500)': make_pipeline(\n",
    "        CountVectorizer(), TruncatedSVD(500), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer (ngrams: 1,2)': make_pipeline(\n",
    "        CountVectorizer(ngram_range=(1,2)), LogisticRegression()\n",
    "    ),\n",
    "    'Count Vectorizer, binary': make_pipeline(\n",
    "        CountVectorizer(binary=True), LogisticRegression()\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd772aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for model_desc, model in model_grid.items():\n",
    "    if model_desc not in model_performance:\n",
    "        model.fit(train_x, train_y)\n",
    "        predictions = model.predict(valid_x)\n",
    "        model_performance[model_desc] = get_performance(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccdcd0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer, binary</th>\n",
       "      <td>0.961081</td>\n",
       "      <td>0.754127</td>\n",
       "      <td>0.649380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer</th>\n",
       "      <td>0.960706</td>\n",
       "      <td>0.753925</td>\n",
       "      <td>0.647191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer (15000 features)</th>\n",
       "      <td>0.960330</td>\n",
       "      <td>0.750428</td>\n",
       "      <td>0.641808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer (10000 features)</th>\n",
       "      <td>0.960205</td>\n",
       "      <td>0.750360</td>\n",
       "      <td>0.641084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer (ngrams: 1,2)</th>\n",
       "      <td>0.960706</td>\n",
       "      <td>0.744038</td>\n",
       "      <td>0.637413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer (5000 features)</th>\n",
       "      <td>0.958829</td>\n",
       "      <td>0.741380</td>\n",
       "      <td>0.624857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF (5000 features)</th>\n",
       "      <td>0.952321</td>\n",
       "      <td>0.672787</td>\n",
       "      <td>0.505837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF (10000 features)</th>\n",
       "      <td>0.950444</td>\n",
       "      <td>0.661066</td>\n",
       "      <td>0.478947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer + TruncatedSVD(500)</th>\n",
       "      <td>0.947816</td>\n",
       "      <td>0.666245</td>\n",
       "      <td>0.476788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer (1000 features)</th>\n",
       "      <td>0.946690</td>\n",
       "      <td>0.659871</td>\n",
       "      <td>0.462121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF</th>\n",
       "      <td>0.949318</td>\n",
       "      <td>0.650573</td>\n",
       "      <td>0.456376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF (1000 features)</th>\n",
       "      <td>0.947065</td>\n",
       "      <td>0.651010</td>\n",
       "      <td>0.448501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count Vectorizer + TruncatedSVD(150)</th>\n",
       "      <td>0.937555</td>\n",
       "      <td>0.598929</td>\n",
       "      <td>0.315501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF (100 features)</th>\n",
       "      <td>0.932674</td>\n",
       "      <td>0.546042</td>\n",
       "      <td>0.167183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline: all zeros</th>\n",
       "      <td>0.929796</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      accuracy  balanced accuracy        F1\n",
       "Count Vectorizer, binary              0.961081           0.754127  0.649380\n",
       "Count Vectorizer                      0.960706           0.753925  0.647191\n",
       "Count Vectorizer (15000 features)     0.960330           0.750428  0.641808\n",
       "Count Vectorizer (10000 features)     0.960205           0.750360  0.641084\n",
       "Count Vectorizer (ngrams: 1,2)        0.960706           0.744038  0.637413\n",
       "Count Vectorizer (5000 features)      0.958829           0.741380  0.624857\n",
       "TF-IDF (5000 features)                0.952321           0.672787  0.505837\n",
       "TF-IDF (10000 features)               0.950444           0.661066  0.478947\n",
       "Count Vectorizer + TruncatedSVD(500)  0.947816           0.666245  0.476788\n",
       "Count Vectorizer (1000 features)      0.946690           0.659871  0.462121\n",
       "TF-IDF                                0.949318           0.650573  0.456376\n",
       "TF-IDF (1000 features)                0.947065           0.651010  0.448501\n",
       "Count Vectorizer + TruncatedSVD(150)  0.937555           0.598929  0.315501\n",
       "TF-IDF (100 features)                 0.932674           0.546042  0.167183\n",
       "Baseline: all zeros                   0.929796           0.500000  0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    pd.DataFrame(model_performance).T\n",
    "    .set_axis(['accuracy', 'balanced accuracy', 'F1'], axis='columns')\n",
    "    .sort_values('F1', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2163a",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- для данной задачи CountVectorizer сработал лучше, чем TF-IDF\n",
    "- увеличение количества признаков идет на пользу CountVectorizer, а в случае TF-IDF - до определенного предела (нужно подбирать)\n",
    "- интересно, что бинарный CountVectorizer (binary=True) сработал чуть лучше, чем вариант со счетчиком\n",
    "- применение PCA (точнее, TruncatedSVD) ухудшило метрику\n",
    "- включение ngram эффекта в данной задаче не принесло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370f561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
